/**
 * Python ↔ TypeScript Interoperability Tests
 * ==========================================
 *
 * Validates that TypeScript produces identical results to Python for:
 * 1. Sacred Tongue encoding/decoding
 * 2. Section → Tongue mapping
 * 3. Token bijectivity
 * 4. PBKDF2 key derivation
 *
 * Test vectors are generated by: python tests/interop/generate_vectors.py
 */

import { describe, it, expect } from 'vitest';
import { readFileSync } from 'fs';
import { join } from 'path';
import { pbkdf2Sync } from 'crypto';
import { SacredTongueTokenizer, TOKENIZER } from '../../src/spiralverse/rwp_v3.js';
import { TONGUES, SECTION_TONGUES } from '../../src/harmonic/sacredTongues.js';

// Load test vectors generated by Python
const vectorsPath = join(__dirname, 'test_vectors.json');
const vectors = JSON.parse(readFileSync(vectorsPath, 'utf-8'));

describe('Python ↔ TypeScript Interoperability', () => {
  describe('Sacred Tongue Encoding', () => {
    const encodeVectors = vectors.sacred_tongue_encode as Array<{
      type: string;
      tongue: string;
      input_hex: string;
      expected_tokens: string[];
      description: string;
    }>;

    it('should have test vectors', () => {
      expect(encodeVectors.length).toBeGreaterThan(0);
    });

    it.each(encodeVectors.map((v, i) => [v.description, v, i]))('%s', (_, vector) => {
      const input = Buffer.from(vector.input_hex, 'hex');
      const tokens = TOKENIZER.encodeBytes(vector.tongue, input);
      expect(tokens).toEqual(vector.expected_tokens);
    });
  });

  describe('Section → Tongue Mapping', () => {
    const sectionVectors = vectors.section_mapping as Array<{
      type: string;
      section: string;
      expected_tongue: string;
      input_hex: string;
      expected_tokens: string[];
      description: string;
    }>;

    it('should have test vectors', () => {
      expect(sectionVectors.length).toBeGreaterThan(0);
    });

    it.each(sectionVectors.map((v) => [v.description, v]))('%s', (_, vector) => {
      // Check tongue mapping
      const actualTongue = SECTION_TONGUES[vector.section as keyof typeof SECTION_TONGUES];
      expect(actualTongue).toBe(vector.expected_tongue);

      // Check token encoding
      const input = Buffer.from(vector.input_hex, 'hex');
      const tokens = TOKENIZER.encodeSection(vector.section, input);
      expect(tokens).toEqual(vector.expected_tokens);
    });
  });

  describe('Bijectivity (Roundtrip)', () => {
    const bijectivityVectors = vectors.bijectivity as Array<{
      type: string;
      tongue: string;
      byte_value: number;
      token: string;
      roundtrip_hex: string;
      is_equal: boolean;
    }>;

    it('should have test vectors', () => {
      expect(bijectivityVectors.length).toBeGreaterThan(0);
    });

    it.each(bijectivityVectors.map((v) => [`${v.tongue} byte ${v.byte_value}`, v]))(
      '%s',
      (_, vector) => {
        // Encode byte
        const input = Buffer.from([vector.byte_value]);
        const tokens = TOKENIZER.encodeBytes(vector.tongue, input);
        expect(tokens.length).toBe(1);
        expect(tokens[0]).toBe(vector.token);

        // Decode back
        const decoded = TOKENIZER.decodeTokens(vector.tongue, tokens);
        expect(decoded.toString('hex')).toBe(vector.roundtrip_hex);
        expect(decoded[0]).toBe(vector.byte_value);
      }
    );
  });

  describe('Tongue Specifications', () => {
    const specVectors = vectors.tongue_specs as Array<{
      type: string;
      tongue: string;
      name: string;
      domain: string;
      prefix_count: number;
      suffix_count: number;
      first_prefix: string;
      last_prefix: string;
      first_suffix: string;
      last_suffix: string;
      sample_token_0x00: string;
      sample_token_0xFF: string;
    }>;

    it('should have test vectors', () => {
      expect(specVectors.length).toBe(6); // 6 tongues
    });

    it.each(specVectors.map((v) => [v.tongue, v]))('tongue %s matches Python spec', (_, vector) => {
      const spec = TONGUES[vector.tongue];
      expect(spec).toBeDefined();
      expect(spec.name).toBe(vector.name);
      expect(spec.prefixes.length).toBe(vector.prefix_count);
      expect(spec.suffixes.length).toBe(vector.suffix_count);
      expect(spec.prefixes[0]).toBe(vector.first_prefix);
      expect(spec.prefixes[15]).toBe(vector.last_prefix);
      expect(spec.suffixes[0]).toBe(vector.first_suffix);
      expect(spec.suffixes[15]).toBe(vector.last_suffix);

      // Verify sample tokens
      const token00 = TOKENIZER.encodeBytes(vector.tongue, Buffer.from([0x00]))[0];
      const tokenFF = TOKENIZER.encodeBytes(vector.tongue, Buffer.from([0xff]))[0];
      expect(token00).toBe(vector.sample_token_0x00);
      expect(tokenFF).toBe(vector.sample_token_0xFF);
    });
  });

  describe('PBKDF2 Key Derivation', () => {
    const pbkdf2Vectors = vectors.pbkdf2 as Array<{
      type: string;
      password_hex: string;
      salt_hex: string;
      iterations: number;
      key_length: number;
      expected_key_hex: string;
      description: string;
    }>;

    it('should have test vectors', () => {
      expect(pbkdf2Vectors.length).toBeGreaterThan(0);
    });

    it.each(pbkdf2Vectors.map((v) => [v.description, v]))('%s', (_, vector) => {
      const password = Buffer.from(vector.password_hex, 'hex');
      const salt = Buffer.from(vector.salt_hex, 'hex');

      const key = pbkdf2Sync(password, salt, vector.iterations, vector.key_length, 'sha256');

      expect(key.toString('hex')).toBe(vector.expected_key_hex);
    });
  });

  describe('Summary', () => {
    it('should have processed all expected vectors', () => {
      const summary = vectors.summary;
      expect(summary.total_vectors).toBe(93);
      expect(summary.sacred_tongue_encode).toBe(42);
      expect(summary.section_mapping).toBe(6);
      expect(summary.bijectivity).toBe(36);
      expect(summary.tongue_specs).toBe(6);
      expect(summary.pbkdf2).toBe(3);
    });
  });
});

describe('TypeScript → Python Vector Generation', () => {
  /**
   * These tests generate vectors that Python can validate.
   * Run: npx vitest run tests/interop/interop.test.ts
   * Then validate in Python with test_interop_ts_vectors.py
   */

  it('should generate consistent tokens for all byte values', () => {
    // This test documents the expected output for validation
    for (const tongueCode of Object.keys(TONGUES)) {
      const tokens: string[] = [];
      for (let b = 0; b < 256; b++) {
        const token = TOKENIZER.encodeBytes(tongueCode, Buffer.from([b]))[0];
        tokens.push(token);
      }

      // Verify uniqueness
      const uniqueTokens = new Set(tokens);
      expect(uniqueTokens.size).toBe(256);

      // Verify format
      for (const token of tokens) {
        expect(token).toContain("'");
        expect(token.split("'").length).toBe(2);
      }
    }
  });

  it('should decode Python-encoded tokens correctly', () => {
    // Manually verify a few known tokens from Python (v1.1 lexicons)
    const pythonTokens = {
      ko: { token: "kor'ah", byte: 0x00 },
      av: { token: "saina'a", byte: 0x00 },
      ru: { token: "khar'ak", byte: 0x00 },
      ca: { token: "bip'a", byte: 0x00 },
      um: { token: "veil'a", byte: 0x00 },
      dr: { token: "anvil'a", byte: 0x00 },
    };

    for (const [tongue, expected] of Object.entries(pythonTokens)) {
      const decoded = TOKENIZER.decodeTokens(tongue, [expected.token]);
      expect(decoded[0]).toBe(expected.byte);
    }
  });
});
