name: Daily Review

on:
  schedule:
    - cron: '0 6 * * *'
  workflow_dispatch:

permissions:
  contents: read
  issues: write

jobs:
  daily-review:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'
          cache: npm

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install Node dependencies
        continue-on-error: true
        run: npm ci

      - name: Install Python dependencies
        continue-on-error: true
        run: |
          python -m pip install --upgrade pip
          if [ -f requirements.txt ]; then
            pip install -r requirements.txt
          fi

      - name: Run JavaScript tests
        id: npm_test
        continue-on-error: true
        run: npm test

      - name: Run Python tests
        id: pytest
        continue-on-error: true
        run: |
          if command -v pytest &>/dev/null && [ -d tests/ ]; then
            pytest tests/ -v --ignore=tests/node_modules 2>&1 || true
          else
            echo "::notice::pytest not available or tests/ missing"
          fi

      - name: Run lint
        id: lint
        continue-on-error: true
        run: npm run lint

      - name: Run type check
        id: typecheck
        continue-on-error: true
        run: npm run typecheck

      - name: Run coherence check
        id: coherence
        if: always()
        continue-on-error: true
        run: |
          if [ ! -f scripts/coherence-check.py ]; then
            echo '{"coherence": 0.0, "status": "skip"}' > coherence-report.json
            echo "::notice::coherence-check.py not found, skipping"
            exit 0
          fi
          npm_outcome='${{ steps.npm_test.outcome }}'
          pytest_outcome='${{ steps.pytest.outcome }}'
          lint_outcome='${{ steps.lint.outcome }}'
          type_outcome='${{ steps.typecheck.outcome }}'
          test_score=0; [ "$npm_outcome" = "success" ] && [ "$pytest_outcome" = "success" ] && test_score=1
          lint_score=0; [ "$lint_outcome" = "success" ] && lint_score=1
          type_score=0; [ "$type_outcome" = "success" ] && type_score=1
          python scripts/coherence-check.py \
            --threshold 0.97 \
            --test-pass-rate "$test_score" \
            --lint-score "$lint_score" \
            --type-coverage "$type_score" \
            --json-path coherence-report.json

      - name: Export coherence outputs
        id: coherence_summary
        if: always()
        run: |
          if [ ! -f coherence-report.json ]; then
            echo '{"coherence": 0.0, "status": "error"}' > coherence-report.json
          fi
          python3 - <<'PY'
          import json, os
          from pathlib import Path
          data = json.loads(Path('coherence-report.json').read_text())
          with open(os.environ['GITHUB_OUTPUT'], 'a', encoding='utf-8') as fh:
              fh.write(f"coherence={data.get('coherence', 0.0)}\n")
              fh.write(f"coherence_status={data.get('status', 'error')}\n")
          PY

      - name: Build review summary
        id: summary
        if: always()
        run: |
          npm_result='${{ steps.npm_test.outcome }}'
          pytest_result='${{ steps.pytest.outcome }}'
          lint_result='${{ steps.lint.outcome }}'
          typecheck_result='${{ steps.typecheck.outcome }}'
          coherence_status='${{ steps.coherence_summary.outputs.coherence_status }}'
          echo "npm_result=$npm_result" >> "$GITHUB_OUTPUT"
          echo "pytest_result=$pytest_result" >> "$GITHUB_OUTPUT"
          echo "lint_result=$lint_result" >> "$GITHUB_OUTPUT"
          echo "typecheck_result=$typecheck_result" >> "$GITHUB_OUTPUT"
          echo "coherence_status=$coherence_status" >> "$GITHUB_OUTPUT"
          if [ "$npm_result" != "success" ] || [ "$pytest_result" != "success" ] || \
             ([ "$coherence_status" != "pass" ] && [ "$coherence_status" != "skip" ]); then
            echo "tests_failed=true" >> "$GITHUB_OUTPUT"
          else
            echo "tests_failed=false" >> "$GITHUB_OUTPUT"
          fi

      - name: Write job summary
        if: always()
        run: |
          echo "## Daily Review - $(date -u +%Y-%m-%d)" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "| Check | Result |" >> $GITHUB_STEP_SUMMARY
          echo "|-------|--------|" >> $GITHUB_STEP_SUMMARY
          echo "| npm test | **${{ steps.summary.outputs.npm_result }}** |" >> $GITHUB_STEP_SUMMARY
          echo "| pytest | **${{ steps.summary.outputs.pytest_result }}** |" >> $GITHUB_STEP_SUMMARY
          echo "| lint | **${{ steps.summary.outputs.lint_result }}** |" >> $GITHUB_STEP_SUMMARY
          echo "| typecheck | **${{ steps.summary.outputs.typecheck_result }}** |" >> $GITHUB_STEP_SUMMARY
          echo "| coherence | **${{ steps.coherence_summary.outputs.coherence }}** (${{ steps.coherence_summary.outputs.coherence_status }}) |" >> $GITHUB_STEP_SUMMARY

      - name: Create issue for failures
        if: always() && steps.summary.outputs.tests_failed == 'true'
        uses: actions/github-script@v7
        with:
          script: |
            const today = new Date().toISOString().slice(0, 10);
            const title = `Daily Review Failure - ${today}`;
            const existing = await github.rest.issues.listForRepo({
              owner: context.repo.owner,
              repo: context.repo.repo,
              labels: 'daily-review',
              state: 'open',
            });
            const dupe = existing.data.find(i => i.title === title);
            if (dupe) {
              console.log(`Issue already exists: #${dupe.number}`);
              return;
            }
            const body = [
              '## Daily Review Summary',
              '',
              '| Check | Result |',
              '|-------|--------|',
              `| npm test | **${{ steps.summary.outputs.npm_result }}** |`,
              `| pytest | **${{ steps.summary.outputs.pytest_result }}** |`,
              `| lint | **${{ steps.summary.outputs.lint_result }}** |`,
              `| typecheck | **${{ steps.summary.outputs.typecheck_result }}** |`,
              `| coherence | **${{ steps.coherence_summary.outputs.coherence }}** (${{ steps.coherence_summary.outputs.coherence_status }}) |`,
              '',
              'Auto-created by the Daily Review workflow.',
            ].join('\n');
            await github.rest.issues.create({
              owner: context.repo.owner,
              repo: context.repo.repo,
              title,
              body,
              labels: ['daily-review', 'automated'],
            });

      - name: Fail job if tests failed
        if: always() && steps.summary.outputs.tests_failed == 'true'
        run: |
          echo "::error::Daily review detected failing checks"
          exit 1
